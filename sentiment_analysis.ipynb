{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the stop words into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stop_words(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Combines all unique stop words from text files in a given directory into a single output file.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Directory containing stop-word text files.\n",
    "        output_file (str): Path to the output file where the combined stop words will be saved.\n",
    "    \"\"\"\n",
    "    # Initialize a set to hold all unique stop words\n",
    "    unique_stop_words = set()\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Check if the file is a .txt file\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    word = line.strip()  # Remove any extra whitespace or newline characters\n",
    "                    unique_stop_words.add(word)  # Add the word to the set\n",
    "\n",
    "    # Write all unique stop words into the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        for word in sorted(unique_stop_words):  # Sort words alphabetically before writing\n",
    "            output.write(word + '\\n')\n",
    "\n",
    "    print(f\"Combined stop-words file created at: {output_file}\")\n",
    "    \n",
    "# Combine all the stop words from the directory into a single file\n",
    "input_directory = \"stop_words\"\n",
    "output_filepath = \"stop_words/combined_stop_words.txt\"\n",
    "combine_stop_words(input_directory, output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AFINN and stopwords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AFINN-111 sentiment lexicon\n",
    "def load_afinn(file_path):\n",
    "    \"\"\"\n",
    "    Load the AFINN-111 lexicon from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the AFINN lexicon.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are words and values are their sentiment scores.\n",
    "    \"\"\"\n",
    "    afinn = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, score = line.strip().split('\\t')\n",
    "            afinn[word] = int(score)\n",
    "    return afinn\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    Load stop words from the given file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing stop words.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of stop words.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set(line.strip() for line in f)\n",
    "    return stopwords\n",
    "\n",
    "# Load AFINN lexicon and stop words\n",
    "affin_file_path = \"AFINN-111.txt\"  # Replace with actual file path\n",
    "stopwords_file_path = \"stop_words/combined_stop_words\"  # Replace with actual file path\n",
    "\n",
    "afinn_lexicon = load_afinn(affin_file_path)\n",
    "stopwords = load_stopwords(stopwords_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Main Algorithm, Neceassary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by removing non-alphanumeric characters and converting to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned and tokenized words.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text).lower().split()\n",
    "\n",
    "def calculate_sentiment(text, afinn):\n",
    "    \"\"\"\n",
    "    Calculate sentiment score of a given text using the AFINN lexicon.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze.\n",
    "        afinn (dict): AFINN lexicon dictionary with words and their sentiment scores.\n",
    "\n",
    "    Returns:\n",
    "        int: The total sentiment score for the text.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    sentiment_score = sum(afinn.get(word, 0) for word in words)\n",
    "    return sentiment_score\n",
    "\n",
    "def classify_sentiment(score):\n",
    "    \"\"\"\n",
    "    Classify sentiment based on the sentiment score.\n",
    "\n",
    "    Args:\n",
    "        score (int): Sentiment score.\n",
    "\n",
    "    Returns:\n",
    "        str: The sentiment category (\"Positive\", \"Negative\", or \"Neutral\").\n",
    "    \"\"\"\n",
    "    if score > 0:\n",
    "        return \"Positive\"\n",
    "    elif score < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: New Term Algortihm, Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_new_terms(text, afinn, stopwords):\n",
    "    \"\"\"\n",
    "    Identify terms not present in the AFINN lexicon and not in the stop words list.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze.\n",
    "        afinn (dict): AFINN lexicon dictionary with words and their sentiment scores.\n",
    "        stopwords (set): Set of stop words to ignore.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of new terms not present in the AFINN lexicon or stop words.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    new_terms = [word for word in words if word not in afinn and word not in stopwords]\n",
    "    return new_terms\n",
    "\n",
    "def assign_new_term_sentiment(text, afinn, stopwords):\n",
    "    \"\"\"\n",
    "    Assign sentiment to new terms based on the overall sentiment of the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze.\n",
    "        afinn (dict): AFINN lexicon dictionary with words and their sentiment scores.\n",
    "        stopwords (set): Set of stop words to ignore.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of new terms and their assigned sentiment values.\n",
    "\n",
    "    Explanation:\n",
    "        - The overall sentiment score is divided by the number of new terms to distribute sentiment proportionally.\n",
    "        - If there are no new terms, the sentiment value defaults to 0.\n",
    "        - This ensures a fair distribution of sentiment among new terms.\n",
    "    \"\"\"\n",
    "    new_terms = identify_new_terms(text, afinn, stopwords)\n",
    "    sentiment_score = calculate_sentiment(text, afinn)\n",
    "\n",
    "    # Divide sentiment score by the number of new terms to distribute sentiment proportionally\n",
    "    # If there are no new terms, default sentiment value to 0\n",
    "    sentiment_value = sentiment_score / max(1, len(new_terms)) if new_terms else 0\n",
    "\n",
    "    # Return a dictionary of new terms and their assigned sentiment value (rounded to 2 decimals)\n",
    "    return {term: round(sentiment_value, 2) for term in new_terms}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input/output texts\n",
    "texts = [\n",
    "    \"I really like new book of that author.\",\n",
    "    \"I hate new regulations about importing policies.\",\n",
    "    \"Look at that door, itâ€™s still open.\"\n",
    "]\n",
    "\n",
    "# Analyze new terms and assign sentiment\n",
    "new_terms_results = []\n",
    "for text in texts:\n",
    "    new_terms_sentiment = assign_new_term_sentiment(text, afinn_lexicon, stopwords)\n",
    "    for term, sentiment in new_terms_sentiment.items():\n",
    "        new_terms_results.append((text, term, sentiment))\n",
    "\n",
    "# Create DataFrame for display\n",
    "new_terms_df = pd.DataFrame(new_terms_results, columns=[\"Text\", \"New Term\", \"Sentiment\"])\n",
    "print(new_terms_df)\n",
    "\n",
    "# Save results to CSV\n",
    "new_terms_df.to_csv(\"sentiments_new.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
